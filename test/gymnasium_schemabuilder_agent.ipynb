{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Add the project root directory to Python path\n",
    "# project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "# sys.path.append(project_root)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayesha.amjad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import gymnasium as gym\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.LLM_utils import get_completion_gpt4\n",
    "from utils.jsonparser_utils import clean_llm_output, json_to_dataframe\n",
    "from actor_agents.schema_builder import schema_building_with_llm\n",
    "from src.action_space.meta_prompting_agent import adjust_prompt\n",
    "from evaluation.scoring import calculate_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Best Score Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaBuilderEnv(gym.Env):\n",
    "    def __init__(self, baseprompt, document_text, groundtruth):\n",
    "        super(SchemaBuilderEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(5)  # 5 possible prompt adjustments\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, \n",
    "            high=float('inf'),  # Perplexity can be any positive number\n",
    "            shape=(1,),  # [Perplexity]\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Store schema building related data\n",
    "        self.baseprompt = baseprompt\n",
    "        self.document_text = document_text\n",
    "        self.groundtruth = groundtruth\n",
    "        self.current_prompt = baseprompt\n",
    "        self.task_type = \"json schema-building\"\n",
    "        \n",
    "        # Track best scores and results\n",
    "        self.best_perplexity = float('inf')\n",
    "        self.best_schema = None\n",
    "        self.best_prompt = None\n",
    "        \n",
    "        # Track consecutive non-improvements\n",
    "        self.non_improvement_count = 0\n",
    "        self.max_non_improvements = 2\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.state = None\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        print(f\"\\n............................. ITERATION {self.current_step} BEGINS.....................................\")\n",
    "\n",
    "        # Get updated prompt using meta-prompting agent\n",
    "        updated_prompt = adjust_prompt(\n",
    "            actor_prompt=self.current_prompt,\n",
    "            task_type=self.task_type,\n",
    "            state=self.state,\n",
    "            action=action,\n",
    "            groundtruth=self.groundtruth,\n",
    "            generated_output=self.last_output if hasattr(self, 'last_output') else None\n",
    "        )\n",
    "        self.current_prompt = updated_prompt\n",
    "\n",
    "        # Generate new schema and get scores\n",
    "        _, self.last_output, perplexity_score = schema_building_with_llm(updated_prompt, self.document_text)\n",
    "\n",
    "        \n",
    "        print(f\"\\nStep {self.current_step}\")\n",
    "        print(f\"\\nUpdated Prompt: {_}\")\n",
    "        print(\"Updated Schema:\\n\", self.last_output)\n",
    "\n",
    "        # Update state (lower perplexity is better)\n",
    "        self.state = np.array([perplexity_score], dtype=np.float32)\n",
    "        \n",
    "        # Check if current scores are better\n",
    "        # Check if current score is better than best score\n",
    "        if perplexity_score < self.best_perplexity:\n",
    "            self.best_perplexity = perplexity_score\n",
    "            self.best_schema = self.last_output\n",
    "            self.best_prompt = self.current_prompt\n",
    "            self.non_improvement_count = 0\n",
    "            improved = True\n",
    "        else:\n",
    "            self.non_improvement_count += 1\n",
    "            improved = False\n",
    "\n",
    "        # Calculate reward (negative because we want to minimize perplexity)\n",
    "        reward = self.best_perplexity - perplexity_score\n",
    "\n",
    "        # Determine if we should terminate\n",
    "        done = self.non_improvement_count >= self.max_non_improvements\n",
    "\n",
    "        # Create info dictionary\n",
    "        info = {\n",
    "            'perplexity': perplexity_score,\n",
    "            'best_perplexity': self.best_perplexity,\n",
    "            'improved': improved,\n",
    "            'non_improvement_count': self.non_improvement_count,\n",
    "            'steps': self.current_step\n",
    "        }\n",
    "\n",
    "        print(f\"Current Perplexity: {perplexity_score:.4f}\")\n",
    "        print(f\"Best Perplexity: {self.best_perplexity:.4f}\")\n",
    "        print(f\"Improved: {improved}\")\n",
    "        print(f\"Non-improvement count: {self.non_improvement_count}\")\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\nTerminating due to no improvements in last two updates\")\n",
    "            print(f\"Best Results Achieved:\")\n",
    "            print(f\"Perplexity: {self.best_perplexity:.4f}\")\n",
    "            print(\"\\nBest Schema:\")\n",
    "            print(self.best_schema)\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.non_improvement_count = 0\n",
    "        self.current_prompt = self.baseprompt\n",
    "        \n",
    "        # Generate initial schema and calculate scores\n",
    "        _, self.last_output, perplexity_score = schema_building_with_llm(self.current_prompt, self.document_text)\n",
    "        \n",
    "        print(f\"\\nInitial Prompt: {_}\")\n",
    "        print(\"Initial Schema:\\n\", self.last_output)\n",
    "        print(f\"Initial Perplexity: {perplexity_score:.4f}\")\n",
    "\n",
    "        # Initialize best scores\n",
    "        self.best_perplexity = perplexity_score\n",
    "        self.best_schema = self.last_output\n",
    "        self.best_prompt = self.current_prompt\n",
    "        \n",
    "        self.state = np.array([perplexity_score], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "    \n",
    "    def get_best_results(self):\n",
    "        \"\"\"Return the best results achieved during the episode\"\"\"\n",
    "        return {\n",
    "            'best_perplexity': self.best_perplexity,\n",
    "            'best_schema': self.best_schema,\n",
    "            'best_prompt': self.best_prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaBuilderEnv(gym.Env):\n",
    "    def __init__(self, baseprompt, document_text, groundtruth):\n",
    "        super(SchemaBuilderEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(5)  # 5 possible prompt adjustments\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0.0, 0.0]),  # [Perplexity, Similarity]\n",
    "            high=np.array([float('inf'), 1.0]),  # Max perplexity is inf, max similarity is 1\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Store schema building related data\n",
    "        self.baseprompt = baseprompt\n",
    "        self.document_text = document_text\n",
    "        self.groundtruth = groundtruth\n",
    "        self.current_prompt = baseprompt\n",
    "        self.task_type = \"json schema-building\"\n",
    "        \n",
    "        # Track best scores and results\n",
    "        self.best_perplexity = float('inf')\n",
    "        self.best_similarity = 0.0\n",
    "        self.best_schema = None\n",
    "        self.best_prompt = None\n",
    "        \n",
    "        # Track consecutive non-improvements\n",
    "        self.non_improvement_count = 0\n",
    "        self.max_non_improvements = 2\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.state = None\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        print(f\"\\n............................. ITERATION {self.current_step} BEGINS.....................................\")\n",
    "\n",
    "        # Get updated prompt using meta-prompting agent\n",
    "        updated_prompt = adjust_prompt(\n",
    "            actor_prompt=self.current_prompt,\n",
    "            task_type=self.task_type,\n",
    "            state=self.state,\n",
    "            action=action,\n",
    "            groundtruth=self.groundtruth,\n",
    "            generated_output=self.last_output if hasattr(self, 'last_output') else None\n",
    "        )\n",
    "        self.current_prompt = updated_prompt\n",
    "\n",
    "        # Generate new schema and get scores\n",
    "        _, self.last_output, perplexity_score = schema_building_with_llm(updated_prompt, self.document_text)\n",
    "        similarity_score = calculate_similarity(self.last_output, self.groundtruth)\n",
    "        \n",
    "        print(f\"\\nStep {self.current_step}\")\n",
    "        print(f\"\\nUpdated Prompt: {_}\")\n",
    "        print(\"Updated Schema:\\n\", self.last_output)\n",
    "\n",
    "        # Update state with both metrics\n",
    "        self.state = np.array([perplexity_score, similarity_score], dtype=np.float32)\n",
    "        \n",
    "        # Check if current scores are better\n",
    "        improved = False\n",
    "        if (perplexity_score < self.best_perplexity or similarity_score >= self.best_similarity) or \\\n",
    "           (perplexity_score <= self.best_perplexity or similarity_score > self.best_similarity):\n",
    "            self.best_perplexity = perplexity_score\n",
    "            self.best_similarity = similarity_score\n",
    "            self.best_schema = self.last_output\n",
    "            self.best_prompt = self.current_prompt\n",
    "            self.non_improvement_count = 0\n",
    "            improved = True\n",
    "        else:\n",
    "            self.non_improvement_count += 1\n",
    "\n",
    "        # Calculate combined reward\n",
    "        perplexity_reward = self.best_perplexity - perplexity_score\n",
    "        similarity_reward = similarity_score - self.best_similarity\n",
    "        reward = perplexity_reward + similarity_reward\n",
    "\n",
    "        # Determine if we should terminate\n",
    "        done = self.non_improvement_count >= self.max_non_improvements\n",
    "\n",
    "        # Create info dictionary\n",
    "        info = {\n",
    "            'perplexity': perplexity_score,\n",
    "            'similarity': similarity_score,\n",
    "            'best_perplexity': self.best_perplexity,\n",
    "            'best_similarity': self.best_similarity,\n",
    "            'improved': improved,\n",
    "            'non_improvement_count': self.non_improvement_count,\n",
    "            'steps': self.current_step\n",
    "        }\n",
    "\n",
    "        print(f\"Current Perplexity: {perplexity_score:.4f}\")\n",
    "        print(f\"Current Similarity: {similarity_score:.4f}\")\n",
    "        print(f\"Best Perplexity: {self.best_perplexity:.4f}\")\n",
    "        print(f\"Best Similarity: {self.best_similarity:.4f}\")\n",
    "        print(f\"Improved: {improved}\")\n",
    "        print(f\"Non-improvement count: {self.non_improvement_count}\")\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\nTerminating due to no improvements in last two updates\")\n",
    "            print(f\"Best Results Achieved:\")\n",
    "            print(f\"Perplexity: {self.best_perplexity:.4f}\")\n",
    "            print(f\"Similarity: {self.best_similarity:.4f}\")\n",
    "            print(\"\\nBest Schema:\")\n",
    "            print(self.best_schema)\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.non_improvement_count = 0\n",
    "        self.current_prompt = self.baseprompt\n",
    "        \n",
    "        # Generate initial schema and calculate scores\n",
    "        _, self.last_output, perplexity_score = schema_building_with_llm(self.current_prompt, self.document_text)\n",
    "        similarity_score = calculate_similarity(self.last_output, self.groundtruth)\n",
    "        \n",
    "        print(f\"\\nStart Prompt: {_}\")\n",
    "        print(\"Start Schema:\\n\", self.last_output)\n",
    "\n",
    "        # Initialize best scores\n",
    "        self.best_perplexity = perplexity_score\n",
    "        self.best_similarity = similarity_score\n",
    "        self.best_schema = self.last_output\n",
    "        self.best_prompt = self.current_prompt\n",
    "        \n",
    "        self.state = np.array([perplexity_score, similarity_score], dtype=np.float32)\n",
    "        return self.state, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymansium Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymnasiumAgent:\n",
    "\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return env.unwrapped.__doc__\n",
    "\n",
    "    def __init__(self, model, env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.docs = self.get_docs(env)\n",
    "\n",
    "        self.instructions = \"\"\"\n",
    "Your goal is to maximize your return, i.e., the sum of the rewards you receive.\n",
    "I will give you an observation, reward, termination flag, truncation flag, and the return so far, formatted as:\n",
    "\n",
    "Observation: <observation>\n",
    "Reward: <reward>\n",
    "Termination: <termination>\n",
    "Truncation: <truncation>\n",
    "Return: <sum_of_rewards>\n",
    "\n",
    "You will respond with an action, formatted as:\n",
    "\n",
    "Action: <action>\n",
    "\n",
    "where you replace <action> with your actual action.\n",
    "\"\"\"\n",
    "        self.action_parser = RegexParser(\n",
    "            regex=r\"Action: (.*)\", output_keys=[\"action\"],\n",
    "        )\n",
    "\n",
    "    def interact(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not terminated:\n",
    "            # Format observation for better readability\n",
    "            obs_dict = {\n",
    "                'Perplexity': observation[0]\n",
    "            }\n",
    "            print(\"\\nCurrent State:\")\n",
    "            for metric, value in obs_dict.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "            # Generate a response (action) using the model\n",
    "            response = self.model([\n",
    "                SystemMessage(content=self.instructions),\n",
    "                HumanMessage(content=f\"\"\"\n",
    "Current observation: {obs_dict}\n",
    "Current total reward: {total_reward:.4f}\n",
    "Task completed: {terminated}\n",
    "\n",
    "Based on these metrics, what action (0-4) would you take to improve the extraction?\n",
    "Remember to respond ONLY with \"Action: <number>\"\n",
    "\"\"\")\n",
    "            ])\n",
    "\n",
    "            try:\n",
    "                action = int(self.action_parser.parse(response.content)['action'])\n",
    "                if not (0 <= action <= 4):\n",
    "                    raise ValueError(\"Action must be between 0 and 4\")\n",
    "            except (ValueError, KeyError) as e:\n",
    "                print(f\"Invalid response from model: {response.content}\")\n",
    "                print(\"Defaulting to action 0\")\n",
    "                action = 0\n",
    "\n",
    "            action = int(self.action_parser.parse(response.content)['action'])\n",
    "\n",
    "            # Perform action in the environment\n",
    "            observation, reward, terminated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"\\nAction taken: {action}\")\n",
    "            print(f\"Reward: {reward:.4f}\")\n",
    "            print(f\"\\nTerminated: {terminated}\")\n",
    "            print(f\"Total Return: {total_reward:.4f}\")\n",
    "            print(\"Metrics:\", info)\n",
    "\n",
    "        print(\"\\nTask completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded prompt template\n",
      "You are a JSON schema builder. You will receive an unstructured form-like document and you will perform following tasks:\n",
      "\n",
      "1. Extract only the key phrases in key-value pairs, and header names in tables from the document. \n",
      "\n",
      "2. Arrange all keys phrases with data types in the parent region of the json and table header names in the child region.\n",
      "\n",
      "3. Ensure the order of key names and table headers aligns with the original text.\n",
      "\n",
      "4. ONLY include unique column names as they appear in the document, without duplicates or associated data. \n",
      "\n",
      "5. ONLY Respond with the JSON schema.\n",
      "\n",
      "Here is the content of the form-like document:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_prompt_from_file(filename):\n",
    "    \"\"\"Load prompt template from a text file\"\"\"\n",
    "    # Get the absolute path to the project root directory\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    # Construct path to the prompts directory\n",
    "    prompt_path = os.path.join(project_root, 'src', 'actor_agents', 'Prompts', filename)\n",
    "    \n",
    "    try:\n",
    "        with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Prompt file not found at: {prompt_path}\")\n",
    "\n",
    "# Load the base prompt template\n",
    "document_type = 'invoice'\n",
    "try:\n",
    "    base_prompt = load_prompt_from_file('schema_builder_prompt.txt')\n",
    "    print(\"Successfully loaded prompt template\")\n",
    "    print(base_prompt)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading prompt: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT PARAMS TEST\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example initial state and actions\n",
    "    invoice = f\"\"\"\n",
    "-----------------Invoice------------------\n",
    "                              Page 1 of 3\n",
    "\n",
    "Invoice Number: INV-12345\n",
    "Customer: XYZ Corp\n",
    "Invoice Date: 2024-06-01\n",
    "\n",
    "\n",
    "Item    Quantity    Price     Total\n",
    "item_1     5         $100      500\n",
    "item_2     10        $50       500\n",
    "item_3     6         $10       60\n",
    "\n",
    "\t\t\t\t\tSubtotal: 1060\n",
    "\t\t\t\t\tTotal GST: 500\n",
    "\t\t\t\t\tTotal Amount: $1560\n",
    "--------------------------------------------\n",
    "\"\"\"\n",
    "    schema = schema = {\n",
    "    \"invoice_number\": \"string\",\n",
    "    \"customer\": \"string\",\n",
    "    \"invoice_date\": \"yyyy-mm-dd\",\n",
    "    \"sub_total\": \"number\",\n",
    "    \"total_GST\": \"number\",\n",
    "    \"total_amount\": \"number\",\n",
    "    \"Line_Items\": [\n",
    "        {\n",
    "            \"item\": \"string\",\n",
    "            \"quantity\": \"number\",\n",
    "            \"price\": \"number\",\n",
    "            \"total\": \"number\"\n",
    "        }\n",
    "    ] \n",
    "    }\n",
    "   \n",
    "    generated_output = {\n",
    "    \"invoice_number\": \"INV-12345\",\n",
    "    \"invoice_date\": \"2024-06-01\",\n",
    "    \"sub_total\": 1060,\n",
    "    \"total_amount\": 1560, \n",
    "    \"Line_Items\": [\n",
    "        {\n",
    "        \"item\": \"item_1\",\n",
    "        \"quantity\": 5,\n",
    "        \"price\": \"$100\",\n",
    "        \"total\": 500\n",
    "        },\n",
    "        {\n",
    "        \"item\": \"item_2\",\n",
    "        \"quantity\": 10,\n",
    "        \"price\": \"$50\",\n",
    "        \"total\": 500\n",
    "        }\n",
    "    ]\n",
    "    } \n",
    "    groundtruth = {\n",
    "    \"invoice_number\": \"INV-12345\",\n",
    "    \"customer\": \"XYZ Corp\",\n",
    "    \"invoice_date\": \"2024-06-01\",\n",
    "    \"sub_total\": 1060,\n",
    "    \"total_GST\":500,\n",
    "    \"total_amount\": 1560,\n",
    "    \"Line_Items\": [\n",
    "        {\n",
    "        \"item\": \"item_1\",\n",
    "        \"quantity\": 5,\n",
    "        \"price\": 100,\n",
    "        \"total\": 500\n",
    "        },\n",
    "        {\n",
    "        \"item\": \"item_2\",\n",
    "        \"quantity\": 10,\n",
    "        \"price\": 50,\n",
    "        \"total\": 500\n",
    "        },\n",
    "        {\n",
    "        \"item\": \"item_3\",\n",
    "        \"quantity\": 6,\n",
    "        \"price\": 10,\n",
    "        \"total\": 60\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    \n",
    "    # print(document_extractor_agent(base_prompt, document_type, invoice, schema))\n",
    "    # print(calculate_exact_match(generated_output,groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GymnasiumAgent.interact() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m agent \u001b[38;5;241m=\u001b[39m GymnasiumAgent(model\u001b[38;5;241m=\u001b[39mChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m), env\u001b[38;5;241m=\u001b[39menv)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Run the interaction\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# # Get the best results\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# best_results = env.get_best_results()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(\"\\nBest Results:\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(f\"Perplexity: {best_results['best_perplexity']:.4f}\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# print(\"\\nBest Schema:\")\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(best_results['best_schema'])\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: GymnasiumAgent.interact() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = SchemaBuilderEnv(\n",
    "    baseprompt=base_prompt,\n",
    "    document_text=invoice,\n",
    "    groundtruth=schema\n",
    ")\n",
    "agent = GymnasiumAgent(model=ChatOpenAI(temperature=0.2), env=env)\n",
    "\n",
    "# Run the interaction\n",
    "agent.interact()\n",
    "\n",
    "# # Get the best results\n",
    "# best_results = env.get_best_results()\n",
    "# print(\"\\nBest Results:\")\n",
    "# print(f\"Perplexity: {best_results['best_perplexity']:.4f}\")\n",
    "# print(\"\\nBest Schema:\")\n",
    "# print(best_results['best_schema'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
