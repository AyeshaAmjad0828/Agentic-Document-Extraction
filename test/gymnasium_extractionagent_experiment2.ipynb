{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayesha.amjad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.LLM_utils import get_completion_gpt4\n",
    "from actor_agents.Invoice_extractor import generate_invoice_agent, baseline_invoice_agent \n",
    "from actor_agents.document_classifier import classify_document_with_llm\n",
    "from actor_agents.schema_builder import schema_building_with_llm\n",
    "from meta_prompting_agent import adjust_prompt\n",
    "from evaluation.scoring import calculate_exact_match, calculate_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataExtractionEnv(gym.Env):\n",
    "    def __init__(self, invoice, schema, groundtruth):\n",
    "        super(DataExtractionEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(5)  # 5 possible prompt adjustments\n",
    "        # Expanded observation space to include perplexity and linear probability\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, \n",
    "            high=1, \n",
    "            shape=(4,),  # [Exact Match, Similarity, Perplexity Score, Linear Probability]\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Store document extraction related data\n",
    "        self.invoice = invoice\n",
    "        self.schema = schema\n",
    "        self.groundtruth = groundtruth\n",
    "        self.current_prompt = generate_invoice_agent(invoice, schema)\n",
    "        self.task_type = 'form-like document extraction'\n",
    "        \n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def get_perplexity_score(self, response):\n",
    "        \"\"\"Calculate perplexity score from response logprobs\"\"\"\n",
    "        logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "        perplexity_score = np.exp(-np.mean(logprobs))\n",
    "        # Normalize perplexity score to [0,1] range\n",
    "        normalized_perplexity = 1 / (1 + perplexity_score)\n",
    "        return normalized_perplexity\n",
    "\n",
    "    def get_linear_probability(self, response):\n",
    "        \"\"\"Calculate linear probability score\"\"\"\n",
    "        logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "        linear_prob = np.exp(np.sum(logprobs))\n",
    "        # Normalize to [0,1] range\n",
    "        normalized_linear_prob = np.clip(linear_prob, 0, 1)\n",
    "        return normalized_linear_prob\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get updated prompt using meta-prompting agent\n",
    "        updated_prompt = adjust_prompt(\n",
    "            actor_prompt=self.current_prompt,\n",
    "            task_type=self.task_type,\n",
    "            state=self.state,\n",
    "            action=action,\n",
    "            generated_output=self.last_output if hasattr(self, 'last_output') else None,\n",
    "            groundtruth=self.groundtruth\n",
    "        )\n",
    "        self.current_prompt = updated_prompt\n",
    "\n",
    "        # Generate new output using the updated prompt with logprobs enabled\n",
    "        response = get_completion_gpt4(\n",
    "            [{\"role\": \"user\", \"content\": updated_prompt}],\n",
    "            logprobs=True\n",
    "        )\n",
    "        self.last_output = response.choices[0].message.content\n",
    "\n",
    "        # Calculate all scores\n",
    "        exact_match_score = calculate_exact_match(self.last_output, self.groundtruth)\n",
    "        similarity_score = calculate_similarity(self.last_output, self.groundtruth)\n",
    "        perplexity_score = self.get_perplexity_score(response)\n",
    "        linear_prob_score = self.get_linear_probability(response)\n",
    "\n",
    "        # Update state with all scores\n",
    "        self.state = np.array([\n",
    "            exact_match_score,\n",
    "            similarity_score,\n",
    "            perplexity_score,\n",
    "            linear_prob_score\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # Calculate comprehensive reward\n",
    "        reward = (\n",
    "            0.4 * exact_match_score +\n",
    "            0.3 * similarity_score +\n",
    "            0.15 * perplexity_score +\n",
    "            0.15 * linear_prob_score -\n",
    "            abs(action - 2) * 0.05  # Action penalty\n",
    "        )\n",
    "\n",
    "        # Check if task is complete (adjusted thresholds)\n",
    "        done = bool(\n",
    "            exact_match_score >= 0.95 and \n",
    "            similarity_score >= 0.95 and\n",
    "            perplexity_score >= 0.7 and\n",
    "            linear_prob_score >= 0.7\n",
    "        )\n",
    "\n",
    "        info = {\n",
    "            'exact_match': exact_match_score,\n",
    "            'similarity': similarity_score,\n",
    "            'perplexity': perplexity_score,\n",
    "            'linear_probability': linear_prob_score\n",
    "        }\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset prompt to initial state\n",
    "        self.current_prompt = generate_invoice_agent(self.invoice, self.schema)\n",
    "        self.last_output = None\n",
    "        # Initialize state with all four metrics\n",
    "        self.state = np.random.uniform(0.2, 0.4, size=(4,))\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymnasiumAgent:\n",
    "    # ... (previous code remains the same until interact method)\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return env.unwrapped.__doc__\n",
    "\n",
    "    def __init__(self, model, env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.docs = self.get_docs(env)\n",
    "\n",
    "        self.instructions = \"\"\"\n",
    "Your goal is to maximize your return, i.e., the sum of the rewards you receive.\n",
    "I will give you an observation, reward, termination flag, truncation flag, and the return so far, formatted as:\n",
    "\n",
    "Observation: <observation>\n",
    "Reward: <reward>\n",
    "Termination: <termination>\n",
    "Truncation: <truncation>\n",
    "Return: <sum_of_rewards>\n",
    "\n",
    "You will respond with an action, formatted as:\n",
    "\n",
    "Action: <action>\n",
    "\n",
    "where you replace <action> with your actual action.\n",
    "\"\"\"\n",
    "        self.action_parser = RegexParser(\n",
    "            regex=r\"Action: (.*)\", output_keys=[\"action\"],\n",
    "        )\n",
    "\n",
    "    def interact(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not terminated:\n",
    "            # Format observation for better readability\n",
    "            obs_dict = {\n",
    "                'Exact Match': observation[0],\n",
    "                'Similarity': observation[1],\n",
    "                'Perplexity': observation[2],\n",
    "                'Linear Probability': observation[3]\n",
    "            }\n",
    "            print(\"\\nCurrent State:\")\n",
    "            for metric, value in obs_dict.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "            # Generate a response (action) using the model\n",
    "            response = self.model([\n",
    "                SystemMessage(content=self.instructions),\n",
    "                HumanMessage(content=f\"\"\"\n",
    "                    Observation: {obs_dict}\n",
    "                    Reward: {total_reward:.4f}\n",
    "                    Termination: {terminated}\n",
    "                    Truncation: False\n",
    "                    Return: {total_reward:.4f}\n",
    "                \"\"\")\n",
    "            ])\n",
    "\n",
    "            action = int(self.action_parser.parse(response.content)['action'])\n",
    "\n",
    "            # Perform action in the environment\n",
    "            observation, reward, terminated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"\\nAction taken: {action}\")\n",
    "            print(f\"Reward: {reward:.4f}\")\n",
    "            print(f\"Total Return: {total_reward:.4f}\")\n",
    "            print(\"Metrics:\", info)\n",
    "\n",
    "        print(\"\\nTask completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymnasiumAgent:\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return env.unwrapped.__doc__\n",
    "\n",
    "    def __init__(self, model, env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.docs = self.get_docs(env)\n",
    "\n",
    "        self.instructions = \"\"\"\n",
    "Your goal is to maximize your return, i.e., the sum of the rewards you receive.\n",
    "I will give you an observation, reward, termination flag, truncation flag, and the return so far, formatted as:\n",
    "\n",
    "Observation: <observation>\n",
    "Reward: <reward>\n",
    "Termination: <termination>\n",
    "Truncation: <truncation>\n",
    "Return: <sum_of_rewards>\n",
    "\n",
    "You will respond with an action, formatted as:\n",
    "\n",
    "Action: <action>\n",
    "\n",
    "where you replace <action> with your actual action.\n",
    "\"\"\"\n",
    "        self.action_parser = RegexParser(\n",
    "            regex=r\"Action: (.*)\", output_keys=[\"action\"],\n",
    "        )\n",
    "\n",
    "    def interact(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not terminated:\n",
    "            print(f\"Observation: {observation}\")\n",
    "\n",
    "            # Generate a response (action) using the model\n",
    "            response = self.model([\n",
    "                SystemMessage(content=self.instructions),\n",
    "                HumanMessage(content=f\"Observation: {observation}\\nReward: {total_reward}\\nTermination: {terminated}\\nTruncation: False\\nReturn: {total_reward}\")\n",
    "            ])\n",
    "\n",
    "            action = int(self.action_parser.parse(response.content)['action'])\n",
    "\n",
    "            # Perform action in the environment\n",
    "            observation, reward, terminated, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"Action: {action}, Reward: {reward}, Total Return: {total_reward}\")\n",
    "\n",
    "        print(\"Task completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment with example data\n",
    "# Example initial state and actions\n",
    "invoice = f\"\"\"\n",
    "-----------------Invoice------------------\n",
    "                              Page 1 of 3\n",
    "\n",
    "Invoice Number: INV-12345\n",
    "Customer: XYZ Corp\n",
    "Invoice Date: 2024-06-01\n",
    "\n",
    "\n",
    "Item    Quantity    Price     Total\n",
    "item_1     5         $100      500\n",
    "item_2     10        $50       500\n",
    "item_3     6         $10       60\n",
    "\n",
    "\t\t\t\t\tSubtotal: 1060\n",
    "\t\t\t\t\tTotal GST: 500\n",
    "\t\t\t\t\tTotal Amount: $1560\n",
    "--------------------------------------------\n",
    "\"\"\"\n",
    "schema = schema = {\n",
    "\"invoice_number\": \"string\",\n",
    "\"customer\": \"string\",\n",
    "\"invoice_date\": \"yyyy-mm-dd\",\n",
    "\"sub_total\": \"number\",\n",
    "\"total_GST\": \"number\",\n",
    "\"total_amount\": \"number\",\n",
    "\"Line_Items\": [\n",
    "    {\n",
    "        \"item\": \"string\",\n",
    "        \"quantity\": \"number\",\n",
    "        \"price\": \"number\",\n",
    "        \"total\": \"number\"\n",
    "    }\n",
    "] \n",
    "}\n",
    "\n",
    "generated_output = {\n",
    "\"invoice_number\": \"INV-12345\",\n",
    "\"invoice_date\": \"2024-06-01\",\n",
    "\"sub_total\": 1060,\n",
    "\"total_amount\": 1560, \n",
    "\"Line_Items\": [\n",
    "    {\n",
    "    \"item\": \"item_1\",\n",
    "    \"quantity\": 5,\n",
    "    \"price\": \"$100\",\n",
    "    \"total\": 500\n",
    "    },\n",
    "    {\n",
    "    \"item\": \"item_2\",\n",
    "    \"quantity\": 10,\n",
    "    \"price\": \"$50\",\n",
    "    \"total\": 500\n",
    "    }\n",
    "]\n",
    "} \n",
    "groundtruth = {\n",
    "\"invoice_number\": \"INV-12345\",\n",
    "\"customer\": \"XYZ Corp\",\n",
    "\"invoice_date\": \"2024-06-01\",\n",
    "\"sub_total\": 1060,\n",
    "\"total_GST\":500,\n",
    "\"total_amount\": 1560,\n",
    "\"Line_Items\": [\n",
    "    {\n",
    "    \"item\": \"item_1\",\n",
    "    \"quantity\": 5,\n",
    "    \"price\": 100,\n",
    "    \"total\": 500\n",
    "    },\n",
    "    {\n",
    "    \"item\": \"item_2\",\n",
    "    \"quantity\": 10,\n",
    "    \"price\": 50,\n",
    "    \"total\": 500\n",
    "    },\n",
    "    {\n",
    "    \"item\": \"item_3\",\n",
    "    \"quantity\": 6,\n",
    "    \"price\": 10,\n",
    "    \"total\": 60\n",
    "    }\n",
    "]\n",
    "}\n",
    "task_type ='form-like document extraction'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 0.34170806294078704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayesha.amjad\\AppData\\Local\\Temp\\ipykernel_35824\\478757569.py:40: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.model([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Reward: 0.046947020198249756, Total Return: 0.046947020198249756\n",
      "Observation: [0.35548219 0.41337379]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Buy '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m DataExtractionEnv()\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m GymnasiumAgent(model\u001b[38;5;241m=\u001b[39mChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m), env\u001b[38;5;241m=\u001b[39menv)\n\u001b[1;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mGymnasiumAgent.interact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate a response (action) using the model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel([\n\u001b[0;32m     41\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstructions),\n\u001b[0;32m     42\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTermination: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTruncation: False\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReturn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m ])\n\u001b[1;32m---> 45\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Perform action in the environment\u001b[39;00m\n\u001b[0;32m     48\u001b[0m observation, reward, terminated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Buy '"
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = DataExtractionEnv(invoice=invoice, schema=schema, groundtruth=groundtruth)\n",
    "agent = GymnasiumAgent(model=ChatOpenAI(temperature=0.2), env=env)\n",
    "\n",
    "# Run the interaction\n",
    "agent.interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
